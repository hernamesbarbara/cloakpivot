#!/usr/bin/env python3
"""Performance trends generation script for CloakPivot CI/CD pipeline.

This script processes historical performance data to generate trend analysis,
charts, and insights for long-term performance monitoring and optimization.
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

try:
    import matplotlib

    matplotlib.use("Agg")  # Use non-interactive backend for CI
    import matplotlib.pyplot as plt

    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print(
        "Warning: matplotlib not available, chart generation disabled", file=sys.stderr
    )


class PerformanceTrendAnalyzer:
    """Analyze performance trends from historical benchmark data."""

    def __init__(self, output_dir: Path):
        """Initialize analyzer with output directory.

        Args:
            output_dir: Directory to store trend analysis outputs
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def load_performance_data(self, performance_file: Path) -> dict:
        """Load performance data from pytest-benchmark JSON file."""
        if not performance_file.exists():
            raise FileNotFoundError(
                f"Performance data file not found: {performance_file}"
            )

        with open(performance_file) as f:
            data = json.load(f)

        if "benchmarks" not in data:
            raise ValueError(
                "Invalid performance file format: missing 'benchmarks' key"
            )

        return data

    def extract_benchmark_metrics(self, performance_data: dict) -> dict[str, dict]:
        """Extract key metrics from benchmark data."""
        metrics = {}

        for benchmark in performance_data["benchmarks"]:
            name = benchmark["name"]
            stats = benchmark["stats"]

            metrics[name] = {
                "mean": stats["mean"],
                "stddev": stats["stddev"],
                "min": stats["min"],
                "max": stats["max"],
                "rounds": stats["rounds"],
                "timestamp": datetime.now().isoformat(),
            }

        return metrics

    def load_historical_data(
        self, history_dir: Path
    ) -> list[tuple[datetime, dict[str, dict]]]:
        """Load historical performance data from directory structure.

        Expected structure: history_dir/YYYYMMDD/performance-data.json
        """
        historical_data = []

        if not history_dir.exists():
            print(
                f"Warning: History directory not found: {history_dir}", file=sys.stderr
            )
            return historical_data

        for date_dir in sorted(history_dir.iterdir()):
            if not date_dir.is_dir():
                continue

            try:
                # Parse date from directory name (YYYYMMDD format)
                date_str = date_dir.name
                date = datetime.strptime(date_str, "%Y%m%d")

                # Look for performance data file
                perf_files = list(date_dir.glob("*performance*.json"))
                if not perf_files:
                    continue

                perf_file = perf_files[0]  # Use first matching file
                perf_data = self.load_performance_data(perf_file)
                metrics = self.extract_benchmark_metrics(perf_data)

                historical_data.append((date, metrics))

            except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
                print(f"Warning: Skipping {date_dir}: {e}", file=sys.stderr)
                continue

        return historical_data

    def analyze_trends(
        self, historical_data: list[tuple[datetime, dict[str, dict]]]
    ) -> dict:
        """Analyze performance trends from historical data."""
        if len(historical_data) < 2:
            return {"error": "Insufficient historical data for trend analysis"}

        trends = {}
        all_benchmarks = set()

        # Collect all benchmark names
        for _, metrics in historical_data:
            all_benchmarks.update(metrics.keys())

        # Analyze each benchmark
        for benchmark_name in all_benchmarks:
            benchmark_trends = {"data_points": [], "mean_values": [], "dates": []}

            # Extract time series for this benchmark
            for date, metrics in historical_data:
                if benchmark_name in metrics:
                    benchmark_data = metrics[benchmark_name]
                    benchmark_trends["dates"].append(date)
                    benchmark_trends["mean_values"].append(benchmark_data["mean"])
                    benchmark_trends["data_points"].append(benchmark_data)

            if len(benchmark_trends["mean_values"]) < 2:
                continue  # Skip benchmarks with insufficient data

            # Calculate trend statistics
            values = benchmark_trends["mean_values"]

            # Simple linear trend (slope calculation)
            n = len(values)
            x_vals = list(range(n))

            # Calculate slope using least squares
            sum_x = sum(x_vals)
            sum_y = sum(values)
            sum_xy = sum(x * y for x, y in zip(x_vals, values))
            sum_x2 = sum(x * x for x in x_vals)

            if n * sum_x2 - sum_x * sum_x != 0:
                slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
            else:
                slope = 0

            # Trend classification
            recent_mean = sum(values[-3:]) / min(3, len(values))
            early_mean = sum(values[:3]) / min(3, len(values))

            change_pct = (
                ((recent_mean - early_mean) / early_mean * 100) if early_mean > 0 else 0
            )

            if abs(change_pct) < 5:
                trend_status = "stable"
            elif change_pct > 5:
                trend_status = "degrading"
            else:
                trend_status = "improving"

            benchmark_trends.update(
                {
                    "slope": slope,
                    "change_percent": change_pct,
                    "trend_status": trend_status,
                    "recent_mean": recent_mean,
                    "early_mean": early_mean,
                    "volatility": max(values) - min(values) if values else 0,
                }
            )

            trends[benchmark_name] = benchmark_trends

        return trends

    def generate_trend_charts(self, trends: dict) -> list[Path]:
        """Generate trend charts for key performance metrics."""
        if not MATPLOTLIB_AVAILABLE:
            print(
                "Skipping chart generation: matplotlib not available", file=sys.stderr
            )
            return []

        chart_files = []

        # Overall performance overview chart
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle("CloakPivot Performance Trends", fontsize=16)

        # Select key benchmarks for overview
        key_benchmarks = []
        for name in trends.keys():
            if any(
                keyword in name.lower()
                for keyword in [
                    "regression_baseline",
                    "small_document",
                    "round_trip",
                    "batch",
                ]
            ):
                key_benchmarks.append(name)

        key_benchmarks = key_benchmarks[:4]  # Limit to 4 for overview

        for idx, benchmark_name in enumerate(key_benchmarks):
            if idx >= 4:
                break

            ax = axes[idx // 2, idx % 2]
            trend_data = trends[benchmark_name]

            dates = trend_data["dates"]
            values = trend_data["mean_values"]

            ax.plot(dates, values, "o-", linewidth=2, markersize=4)
            ax.set_title(f"{benchmark_name}", fontsize=10)
            ax.set_ylabel("Time (seconds)")
            ax.tick_params(axis="x", rotation=45)

            # Add trend line
            if len(values) > 1:
                x_numeric = list(range(len(values)))
                z = np.polyfit(x_numeric, values, 1)
                p = np.poly1d(z)
                ax.plot(dates, p(x_numeric), "r--", alpha=0.8)

        plt.tight_layout()
        overview_file = self.output_dir / "performance-overview.png"
        plt.savefig(overview_file, dpi=150, bbox_inches="tight")
        plt.close()
        chart_files.append(overview_file)

        # Individual detailed charts for benchmarks with significant trends
        significant_trends = [
            (name, data)
            for name, data in trends.items()
            if abs(data.get("change_percent", 0)) > 10
            or data.get("trend_status") != "stable"
        ]

        for benchmark_name, trend_data in significant_trends[
            :6
        ]:  # Limit to avoid too many files
            fig, ax = plt.subplots(figsize=(10, 6))

            dates = trend_data["dates"]
            values = trend_data["mean_values"]

            ax.plot(dates, values, "o-", linewidth=2, markersize=6)
            ax.set_title(f"Performance Trend: {benchmark_name}")
            ax.set_xlabel("Date")
            ax.set_ylabel("Time (seconds)")

            # Add trend info
            status = trend_data["trend_status"]
            change_pct = trend_data["change_percent"]

            color = {"improving": "green", "degrading": "red", "stable": "blue"}[status]
            ax.text(
                0.02,
                0.98,
                f"Status: {status.title()}\nChange: {change_pct:+.1f}%",
                transform=ax.transAxes,
                verticalalignment="top",
                bbox={"boxstyle": "round", "facecolor": color, "alpha": 0.1},
            )

            plt.xticks(rotation=45)
            plt.tight_layout()

            chart_file = (
                self.output_dir
                / f'trend-{benchmark_name.replace("/", "_").replace(" ", "_")}.png'
            )
            plt.savefig(chart_file, dpi=150, bbox_inches="tight")
            plt.close()
            chart_files.append(chart_file)

        return chart_files

    def generate_trend_report(
        self, trends: dict, current_data: Optional[dict] = None
    ) -> str:
        """Generate markdown trend report."""
        report_lines = []

        report_lines.append("# Performance Trends Analysis")
        report_lines.append(
            f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        )
        report_lines.append("")

        if not trends or "error" in trends:
            report_lines.append("‚ö†Ô∏è **Insufficient historical data for trend analysis**")
            report_lines.append("")
            report_lines.append(
                "Trend analysis requires at least 2 historical data points."
            )
            return "\n".join(report_lines)

        # Summary statistics
        total_benchmarks = len(trends)
        improving = sum(
            1 for t in trends.values() if t.get("trend_status") == "improving"
        )
        degrading = sum(
            1 for t in trends.values() if t.get("trend_status") == "degrading"
        )
        stable = sum(1 for t in trends.values() if t.get("trend_status") == "stable")

        report_lines.append("## Summary")
        report_lines.append(f"- **Total Benchmarks Tracked**: {total_benchmarks}")
        report_lines.append(f"- **Improving Performance**: {improving} üü¢")
        report_lines.append(f"- **Degrading Performance**: {degrading} üî¥")
        report_lines.append(f"- **Stable Performance**: {stable} ‚ö™")
        report_lines.append("")

        # Degrading benchmarks (highest priority)
        if degrading > 0:
            report_lines.append("## üî¥ Degrading Performance Trends")
            report_lines.append(
                "| Benchmark | Change | Trend | Recent Mean | Volatility |"
            )
            report_lines.append(
                "|-----------|---------|-------|-------------|------------|"
            )

            degrading_items = [
                (name, data)
                for name, data in trends.items()
                if data.get("trend_status") == "degrading"
            ]
            degrading_items.sort(
                key=lambda x: x[1].get("change_percent", 0), reverse=True
            )

            for name, data in degrading_items:
                report_lines.append(
                    f"| {name} | +{data['change_percent']:.1f}% | "
                    f"{'üìà' if data['slope'] > 0 else 'üìâ'} | "
                    f"{data['recent_mean']:.4f}s | "
                    f"{data['volatility']:.4f}s |"
                )
            report_lines.append("")

        # Improving benchmarks
        if improving > 0:
            report_lines.append("## üü¢ Improving Performance Trends")
            report_lines.append(
                "| Benchmark | Change | Trend | Recent Mean | Volatility |"
            )
            report_lines.append(
                "|-----------|---------|-------|-------------|------------|"
            )

            improving_items = [
                (name, data)
                for name, data in trends.items()
                if data.get("trend_status") == "improving"
            ]
            improving_items.sort(
                key=lambda x: abs(x[1].get("change_percent", 0)), reverse=True
            )

            for name, data in improving_items:
                report_lines.append(
                    f"| {name} | {data['change_percent']:.1f}% | "
                    f"{'üìà' if data['slope'] > 0 else 'üìâ'} | "
                    f"{data['recent_mean']:.4f}s | "
                    f"{data['volatility']:.4f}s |"
                )
            report_lines.append("")

        # Most volatile benchmarks
        volatile_items = [
            (name, data)
            for name, data in trends.items()
            if data.get("volatility", 0) > 0
        ]
        volatile_items.sort(key=lambda x: x[1].get("volatility", 0), reverse=True)

        if volatile_items:
            report_lines.append("## üìä Most Volatile Performance")
            report_lines.append(
                "*High volatility may indicate inconsistent performance or environmental factors*"
            )
            report_lines.append("")
            report_lines.append("| Benchmark | Volatility | Status | Recent Mean |")
            report_lines.append("|-----------|------------|--------|-------------|")

            for name, data in volatile_items[:5]:  # Top 5 most volatile
                report_lines.append(
                    f"| {name} | {data['volatility']:.4f}s | "
                    f"{data['trend_status'].title()} | "
                    f"{data['recent_mean']:.4f}s |"
                )
            report_lines.append("")

        # Recommendations
        report_lines.append("## üìã Recommendations")

        if degrading > 0:
            report_lines.append(
                "- **Priority**: Investigate degrading performance trends"
            )
            report_lines.append("- Review recent changes that may impact performance")
            report_lines.append(
                "- Consider profiling slow benchmarks for optimization opportunities"
            )

        if improving > 0:
            report_lines.append("- **Great Work**: Performance improvements detected!")
            report_lines.append(
                "- Document optimization techniques for future reference"
            )

        if len(volatile_items) > 3:
            report_lines.append(
                "- **Stability**: High volatility detected in several benchmarks"
            )
            report_lines.append(
                "- Consider increasing benchmark rounds for more stable measurements"
            )
            report_lines.append(
                "- Check for environmental factors affecting consistency"
            )

        if degrading == 0 and improving == 0:
            report_lines.append(
                "- **Status**: Performance appears stable across all metrics"
            )
            report_lines.append("- Continue monitoring for any emerging trends")

        return "\n".join(report_lines)

    def run_analysis(
        self, performance_data: Path, history_dir: Optional[Path] = None
    ) -> dict:
        """Run complete trend analysis."""
        results = {
            "timestamp": datetime.now().isoformat(),
            "charts_generated": [],
            "reports_generated": [],
        }

        try:
            # Load current performance data
            current_data = self.load_performance_data(performance_data)
            results["current_benchmarks"] = len(current_data["benchmarks"])

            # Load historical data if available
            historical_data = []
            if history_dir:
                historical_data = self.load_historical_data(history_dir)

            results["historical_data_points"] = len(historical_data)

            # Add current data to historical data
            current_metrics = self.extract_benchmark_metrics(current_data)
            historical_data.append((datetime.now(), current_metrics))

            # Analyze trends
            trends = self.analyze_trends(historical_data)
            results["trends"] = trends

            # Generate charts
            if MATPLOTLIB_AVAILABLE:
                chart_files = self.generate_trend_charts(trends)
                results["charts_generated"] = [str(f) for f in chart_files]

            # Generate report
            report_content = self.generate_trend_report(trends, current_data)
            report_file = self.output_dir / "performance-trends-report.md"

            with open(report_file, "w") as f:
                f.write(report_content)

            results["reports_generated"].append(str(report_file))

        except Exception as e:
            results["error"] = str(e)

        return results


# Ensure numpy is available for trend line calculations
try:
    import numpy as np

    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

    # Provide simple fallback for polyfit
    class np:
        @staticmethod
        def polyfit(x, y, deg):
            return [0, sum(y) / len(y)] if y else [0, 0]

        @staticmethod
        def poly1d(coeffs):
            return lambda x: [coeffs[0] * xi + coeffs[1] for xi in x]


def main():
    """Main entry point for performance trends generation."""
    parser = argparse.ArgumentParser(
        description="Generate performance trends analysis from benchmark data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --performance-data current.json --output-dir trends/
  %(prog)s --performance-data current.json --history-dir history/ --output-dir trends/
        """,
    )
    parser.add_argument(
        "--performance-data",
        required=True,
        type=Path,
        help="Current performance data JSON file from pytest-benchmark",
    )
    parser.add_argument(
        "--history-dir",
        type=Path,
        help="Directory containing historical performance data (YYYYMMDD subdirs)",
    )
    parser.add_argument(
        "--output-dir",
        required=True,
        type=Path,
        help="Output directory for trend analysis results",
    )
    parser.add_argument(
        "--json-output", action="store_true", help="Output results in JSON format"
    )

    args = parser.parse_args()

    try:
        analyzer = PerformanceTrendAnalyzer(args.output_dir)
        results = analyzer.run_analysis(args.performance_data, args.history_dir)

        if args.json_output:
            print(json.dumps(results, indent=2))
        else:
            print("‚úÖ Trend analysis completed", file=sys.stderr)
            print(f"üìä Output directory: {args.output_dir}", file=sys.stderr)

            if results.get("charts_generated"):
                print(
                    f"üìà Charts generated: {len(results['charts_generated'])}",
                    file=sys.stderr,
                )

            if results.get("reports_generated"):
                print(
                    f"üìã Reports generated: {len(results['reports_generated'])}",
                    file=sys.stderr,
                )

            if "error" in results:
                print(f"‚ö†Ô∏è Error: {results['error']}", file=sys.stderr)
                return 1

        return 0

    except KeyboardInterrupt:
        print("\n‚èπ Analysis cancelled by user", file=sys.stderr)
        return 130
    except Exception as e:
        print(f"üí• Unexpected error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
