name: Cache Performance Metrics

on:
  schedule:
    # Run weekly on Mondays at 6 AM UTC to analyze cache performance
    - cron: '0 6 * * 1'
  workflow_dispatch:
    inputs:
      days:
        description: 'Days of history to analyze'
        required: false
        default: '7'
        type: string
      workflow_name:
        description: 'Workflow to analyze'
        required: false
        default: 'ci.yml'
        type: string

permissions:
  actions: read
  contents: read

jobs:
  analyze-cache-performance:
    name: Analyze Cache Performance
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install analysis dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Generate cache performance report
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        DAYS="${{ github.event.inputs.days || '7' }}"
        WORKFLOW="${{ github.event.inputs.workflow_name || 'ci.yml' }}"
        
        echo "üìä Analyzing cache performance for last ${DAYS} days"
        echo "üîç Workflow: ${WORKFLOW}"
        echo "üìÖ Repository: ${{ github.repository }}"
        
        python scripts/analyze-cache-performance.py \
          --github-token "$GITHUB_TOKEN" \
          --repo "${{ github.repository }}" \
          --workflow "$WORKFLOW" \
          --days "$DAYS" \
          --output-format text \
          --output-file cache-performance-report.txt

    - name: Display cache performance summary
      run: |
        echo "üìã Cache Performance Summary:"
        echo "================================"
        cat cache-performance-report.txt

    - name: Upload cache performance report
      uses: actions/upload-artifact@v4
      with:
        name: cache-performance-report-${{ github.run_number }}
        path: cache-performance-report.txt
        retention-days: 30

    - name: Generate JSON metrics for potential integration
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        DAYS="${{ github.event.inputs.days || '7' }}"
        WORKFLOW="${{ github.event.inputs.workflow_name || 'ci.yml' }}"
        
        python scripts/analyze-cache-performance.py \
          --github-token "$GITHUB_TOKEN" \
          --repo "${{ github.repository }}" \
          --workflow "$WORKFLOW" \
          --days "$DAYS" \
          --output-format json \
          --output-file cache-metrics.json

    - name: Upload JSON metrics
      uses: actions/upload-artifact@v4
      with:
        name: cache-metrics-json-${{ github.run_number }}
        path: cache-metrics.json
        retention-days: 30

    # Optional: Create an issue if cache performance is poor
    - name: Check cache performance thresholds
      id: performance-check
      run: |
        # Extract overall hit rate from JSON metrics
        OVERALL_HIT_RATE=$(python -c "
        import json
        with open('cache-metrics.json', 'r') as f:
            data = json.load(f)
        hit_rate = data['cache_performance']['overall_hit_rate']
        print(f'{hit_rate:.1f}')
        ")
        
        echo "Overall cache hit rate: ${OVERALL_HIT_RATE}%"
        echo "hit-rate=${OVERALL_HIT_RATE}" >> $GITHUB_OUTPUT
        
        # Set threshold for poor performance
        if (( $(echo "$OVERALL_HIT_RATE < 70.0" | bc -l) )); then
          echo "performance-alert=true" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è Cache performance below threshold (70%)"
        else
          echo "performance-alert=false" >> $GITHUB_OUTPUT
          echo "‚úÖ Cache performance meets threshold"
        fi

    - name: Create performance alert issue
      if: steps.performance-check.outputs.performance-alert == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('cache-performance-report.txt', 'utf8');
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `üö® CI Cache Performance Alert - Hit Rate: ${{ steps.performance-check.outputs.hit-rate }}%`,
            body: `## Cache Performance Alert
          
          The automated cache performance analysis has detected suboptimal cache performance:
          
          - **Overall Hit Rate**: ${{ steps.performance-check.outputs.hit-rate }}%
          - **Threshold**: 70%
          - **Analysis Period**: ${{ github.event.inputs.days || '7' }} days
          - **Workflow**: ${{ github.event.inputs.workflow_name || 'ci.yml' }}
          
          ## Performance Report
          
          \`\`\`
          ${report}
          \`\`\`
          
          ## Recommended Actions
          
          1. **Review Cache Keys**: Check if dependency changes are causing frequent cache invalidations
          2. **Verify Cache Paths**: Ensure all model download locations are properly cached
          3. **Check Cache Size Limits**: Verify we're not exceeding GitHub Actions cache limits
          4. **Review Restore Keys**: Consider optimizing fallback cache key strategy
          
          ## Artifacts
          
          - [Cache Performance Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [JSON Metrics](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          This issue was automatically created by the cache performance monitoring workflow.`,
            labels: ['performance', 'ci/cd', 'automated']
          });

  cache-cleanup-analysis:
    name: Cache Storage Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Analyze cache storage usage
      run: |
        echo "üìä Cache Storage Analysis"
        echo "========================"
        
        # Calculate estimated cache usage based on model sizes
        echo "üì¶ Estimated Cache Sizes per Python Version:"
        echo "  spaCy small models: ~50MB"
        echo "  spaCy medium models: ~150MB" 
        echo "  spaCy large models: ~500MB"
        echo "  HuggingFace cache: ~200MB"
        echo "  Pip packages: ~100MB"
        echo ""
        
        # Python versions from matrix
        PYTHON_VERSIONS=("3.9" "3.10" "3.11" "3.12")
        OS_VARIANTS=("ubuntu-latest" "macos-latest" "windows-latest")
        
        TOTAL_ESTIMATED=0
        
        echo "üìã Estimated Cache Usage by Configuration:"
        for PY_VERSION in "${PYTHON_VERSIONS[@]}"; do
          for OS in "${OS_VARIANTS[@]}"; do
            # Estimate based on model size and OS
            ESTIMATED_SIZE_MB=250  # Base estimate for small models + deps
            TOTAL_ESTIMATED=$((TOTAL_ESTIMATED + ESTIMATED_SIZE_MB))
            echo "  Python ${PY_VERSION} on ${OS}: ~${ESTIMATED_SIZE_MB}MB"
          done
        done
        
        echo ""
        echo "üìà Total Estimated Cache Usage: ~${TOTAL_ESTIMATED}MB"
        echo "üìä GitHub Actions Cache Limit: 10GB (10,240MB)"
        
        USAGE_PERCENT=$((TOTAL_ESTIMATED * 100 / 10240))
        echo "üìä Estimated Usage: ${USAGE_PERCENT}% of limit"
        
        if [ $USAGE_PERCENT -gt 80 ]; then
          echo "‚ö†Ô∏è  WARNING: Approaching cache storage limit"
        elif [ $USAGE_PERCENT -gt 50 ]; then
          echo "‚ÑπÔ∏è  INFO: Moderate cache usage, monitor growth"
        else
          echo "‚úÖ Cache usage within safe limits"
        fi